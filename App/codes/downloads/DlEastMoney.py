# -*- coding: utf-8 -*-
import time
import json
import re
from selenium import webdriver
from bs4 import BeautifulSoup as soup
import pandas as pd
from download_utils import page_source
from download_utils import UrlCode
from App.codes.RnnDataFile.parser import my_headers, my_url
import logging
from typing import Optional
from dataclasses import dataclass
from datetime import datetime, time as dt_time

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 5000)

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

@dataclass
class DataValidationConfig:
    """æ•°æ®éªŒè¯é…ç½®"""
    REQUIRED_COLUMNS = ['date', 'open', 'close', 'high', 'low', 'volume', 'money']
    MIN_ROWS = 1
    TRADING_START_TIME = dt_time(9, 30)
    TRADING_END_TIME = dt_time(15, 0)
    

def show_download(freq: str, code: str) -> None:
    """
    ä½¿ç”¨ logging æ˜¾ç¤ºä¸‹è½½æˆåŠŸçš„ä¿¡æ¯ã€‚

    å‚æ•°:
        freq (str): æ•°æ®é¢‘ç‡ï¼Œå¦‚ '1m'ã€‚
        code (str): è‚¡ç¥¨ä»£ç ã€‚
    """
    logging.info(f'Success Download {freq} Data: {code}')


def return_FundsData(source: str, date_new: pd.Timestamp, table_index=1, relevant_columns=None) -> pd.DataFrame:
    """
    ä»ç½‘é¡µæºä»£ç ä¸­æå–å¹¶å¤„ç†èµ„é‡‘æ•°æ®ã€‚

    å‚æ•°: source (str): åŒ…å«HTMLè¡¨æ ¼æ•°æ®çš„ç½‘é¡µæºä»£ç ã€‚ date_new (pd.Timestamp): äº¤æ˜“æ—¥æœŸï¼Œä¼šè½¬æ¢ä¸ºdatetimeæ ¼å¼å¹¶æ·»åŠ åˆ°æ•°æ®æ¡†ä¸­ã€‚ table_index (int,
    optional): æŒ‡å®šè¦è¯»å–çš„è¡¨æ ¼ç´¢å¼•ï¼Œé»˜è®¤å€¼ä¸º1ã€‚ relevant_columns (list of str, optional): è¿”å›çš„æ•°æ®æ¡†ä¸­æ‰€éœ€çš„åˆ—åï¼Œé»˜è®¤å€¼ä¸º['trade_date',
    'stock_code', 'stock_name', 'industry']ã€‚

    è¿”å›:
        pd.DataFrame: åŒ…å«å¤„ç†åçš„èµ„é‡‘æ•°æ®çš„æ•°æ®æ¡†ã€‚
                       åŒ…æ‹¬æŒ‡å®šçš„åˆ—ï¼ˆé»˜è®¤ä¸º['trade_date', 'stock_code', 'stock_name', 'industry']ï¼‰ã€‚
    """

    if relevant_columns is None:
        relevant_columns = ['trade_date', 'stock_code', 'stock_name', 'industry']

    # è¯»å–æŒ‡å®šç´¢å¼•çš„æ•°æ®è¡¨
    df = pd.read_html(source)[table_index]

    # å®šä¹‰æ‰€æœ‰åˆ—å
    columns = ['åºå·', 'stock_code', 'stock_name', 'ç›¸å…³', 'ä»Šæ—¥æ”¶ç›˜ä»·', 'ä»Šæ—¥æ¶¨è·Œå¹…', 'ä»Šæ—¥æŒè‚¡è‚¡æ•°',
               'ä»Šæ—¥æŒè‚¡å¸‚å€¼', 'ä»Šæ—¥æŒè‚¡å æµé€šè‚¡æ¯”', 'ä»Šæ—¥æŒè‚¡å æ€»è‚¡æœ¬æ¯”', 'ä»Šæ—¥å¢æŒè‚¡æ•°',
               'ä»Šæ—¥å¢æŒå¸‚å€¼', 'ä»Šæ—¥å¢æŒå¸‚å€¼å¢å¹…', 'ä»Šæ—¥å¢æŒå æµé€šè‚¡æ¯”', 'ä»Šæ—¥å¢æŒå æ€»è‚¡æœ¬æ¯”', 'industry']

    # ä¸ºDataFrameèµ‹äºˆåˆ—å
    df.columns = columns

    # æ·»åŠ äº¤æ˜“æ—¥æœŸåˆ—
    df['trade_date'] = pd.to_datetime(date_new)

    # åªé€‰æ‹©éœ€è¦çš„åˆ—å¹¶è¿”å›
    result_df = df[relevant_columns]

    # ç¡®ä¿stock_codeä¸ºå­—ç¬¦ä¸²ç±»å‹
    result_df['stock_code'] = result_df['stock_code'].astype(str)

    return result_df


def convert_currency_unit(unit: str):
    """
    è´§å¸å•ä½è½¬æ¢å‡½æ•° (Unit conversion)

    å‚æ•°:
        x (str): è´§å¸å•ä½ï¼Œæ”¯æŒ 'äº¿', 'ä¸‡', 'ç™¾ä¸‡', 'åƒä¸‡' ç­‰ã€‚

    è¿”å›:
        int: å¯¹åº”è´§å¸å•ä½çš„æ•°å€¼ï¼Œå¦‚æœè¾“å…¥å•ä½ä¸åŒ¹é…ï¼Œè¿”å›1ã€‚
    """

    # ä½¿ç”¨å­—å…¸æ˜ å°„å•ä½ä¸æ•°å€¼
    unit_mapping = {
        'äº¿': 100000000,
        'ä¸‡': 10000,
        'ç™¾ä¸‡': 1000000,
        'åƒä¸‡': 10000000}

    # è¿”å›å¯¹åº”çš„æ•°å€¼ï¼Œå¦‚æœå•ä½ä¸åŒ¹é…ï¼Œè¿”å›1

    return unit_mapping.get(unit, 1)


def funds_data_clean(data):
    """
        æ¸…ç†å¹¶è½¬æ¢èµ„é‡‘æ•°æ®ã€‚

        å‚æ•°:
            code_data (pd.DataFrame): è¾“å…¥çš„æ•°æ®æ¡†ï¼ŒåŒ…å«åŸå§‹èµ„é‡‘æ•°æ®ã€‚

        è¿”å›:
            pd.DataFrame: æ¸…ç†å¹¶è½¬æ¢åçš„èµ„é‡‘æ•°æ®ã€‚
        """
    # å°†æ•°æ®è½¬ä¸ºDataFrameå¹¶é€‰æ‹©æ‰€éœ€çš„åˆ—
    data = pd.DataFrame(data.values).iloc[:, [1, 5, 6, 7, 9, 12]]

    # é‡å‘½ååˆ—
    data.columns = ['æ¿å—', 'NkPTå¸‚å€¼', 'NkPTå æ¿å—æ¯”', 'NkPTå åŒ—å‘èµ„é‡‘æ¯”', 'NRPTå¸‚å€¼', 'NRPTå åŒ—å‘èµ„é‡‘æ¯”']

    # å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥å¤„ç†å•ä½è½¬æ¢å’Œæ•°å€¼è®¡ç®—
    def convert_value(value):
        unit = value[-1]  # æå–å•ä½
        number = float(value[:-1])  # æå–æ•°å­—éƒ¨åˆ†å¹¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°
        return number * convert_currency_unit(unit)

    # åº”ç”¨è½¬æ¢å‡½æ•°åˆ°NkPTå¸‚å€¼å’ŒNRPTå¸‚å€¼åˆ—
    data['NkPTå¸‚å€¼'] = data['NkPTå¸‚å€¼'].apply(convert_value)
    data['NRPTå¸‚å€¼'] = data['NRPTå¸‚å€¼'].apply(convert_value)

    return data


def parse_json(source: str, match: bool) -> list:
    """
    è§£æ JSON æ•°æ®ã€‚

    å‚æ•°:
        source (str): åŸå§‹æ•°æ®å­—ç¬¦ä¸²ã€‚
        match (bool): æ˜¯å¦è¿›è¡Œæ­£åˆ™åŒ¹é…ã€‚

    è¿”å›:
        list: è§£æåçš„æ•°æ®åˆ—è¡¨ã€‚å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚
    """
    try:
        if not source:
            logging.error("æ”¶åˆ°ç©ºçš„æºæ•°æ®")
            return []

        if match:
            # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼æ¥æå–JSONæ•°æ®
            pattern = re.compile(r'jQuery\d+_\d+\((.*)\)')
            matches = re.findall(pattern, source)
            if not matches:
                logging.error(f"æ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œæºæ•°æ®å‰200å­—ç¬¦: {source[:200]}...")
                return []
            json_str = matches[0]
            logging.debug(f"æ­£åˆ™åŒ¹é…æˆåŠŸï¼Œæå–çš„JSONå­—ç¬¦ä¸²: {json_str[:200]}...")
        else:
            json_str = source
            
        # å°è¯•è§£æ JSON
        try:
            data = json.loads(json_str)
            logging.debug(f"JSONè§£ææˆåŠŸï¼Œæ•°æ®ç»“æ„: {list(data.keys()) if isinstance(data, dict) else 'not a dict'}")
            print(f"[DEBUG] JSONè§£ææˆåŠŸï¼Œé¡¶å±‚å­—æ®µ: {list(data.keys()) if isinstance(data, dict) else 'not a dict'}")
        except json.JSONDecodeError as e:
            logging.error(f"JSONè§£æé”™è¯¯: {e}, æºæ•°æ®ç‰‡æ®µ: {json_str[:200]}...")
            return []
            
        # æ£€æŸ¥æ•°æ®ç»“æ„
        if not isinstance(data, dict):
            logging.error(f"æ•°æ®æ ¼å¼é”™è¯¯ï¼Œé¢„æœŸdictï¼Œå®é™…: {type(data)}")
            return []
            
        if 'data' not in data:
            logging.error("æ•°æ®ä¸­ç¼ºå°‘'data'å­—æ®µ")
            print(f"[DEBUG] æ•°æ®ä¸­ç¼ºå°‘'data'å­—æ®µï¼Œå¯ç”¨å­—æ®µ: {list(data.keys())}")
            return []
        
        # æ‰“å°dataå­—æ®µçš„è¯¦ç»†ä¿¡æ¯ç”¨äºè°ƒè¯•
        print(f"[DEBUG] dataå­—æ®µç±»å‹: {type(data['data'])}, å†…å®¹: {str(data['data'])[:500] if isinstance(data['data'], dict) else str(data['data'])[:200]}")
        if isinstance(data['data'], dict):
            print(f"[DEBUG] dataå­—æ®µåŒ…å«çš„é”®: {list(data['data'].keys())}")
        
        # æ£€æŸ¥ data['data'] æ˜¯å¦ä¸º None
        if data['data'] is None:
            rc = data.get('rc', 0)
            rt = data.get('rt', 0)
            logging.error(f"dataå­—æ®µä¸ºNoneï¼ŒAPIè¿”å› rc={rc}, rt={rt}")
            print(f"[DEBUG] dataå­—æ®µä¸ºNoneï¼ŒAPIå“åº”: {json.dumps(data, ensure_ascii=False)[:500]}")
            
            # æ£€æŸ¥APIè¿”å›ç 
            # rc: è¿”å›ç ï¼Œ0è¡¨ç¤ºæˆåŠŸï¼Œé0è¡¨ç¤ºé”™è¯¯
            # rt: å“åº”ç±»å‹ï¼Œé€šå¸¸0è¡¨ç¤ºæˆåŠŸ
            if rc != 0 or rt != 0:
                error_msg = f"APIè¿”å›é”™è¯¯: rc={rc}, rt={rt}"
                if rc == 100:
                    error_msg += " (å¯èƒ½æ˜¯æ•°æ®ä¸å­˜åœ¨æˆ–æ¿å—ä»£ç æ— æ•ˆ)"
                logging.error(error_msg)
                print(f"[DEBUG] {error_msg}")
            return []
            
        # å°è¯•ä¸åŒçš„æ•°æ®å­—æ®µåç§°
        # æ³¨æ„ï¼šæ¿å—æ•°æ®å¯èƒ½ä½¿ç”¨ä¸åŒçš„å­—æ®µåï¼Œå¦‚ 'trends' æˆ– 'klines'
        data_fields = ['klines', 'trends', 'data']
        result_data = None
        
        for field in data_fields:
            if isinstance(data['data'], dict) and field in data['data']:
                field_value = data['data'][field]
                # æ£€æŸ¥å­—æ®µå€¼æ˜¯å¦æœ‰æ•ˆï¼ˆéç©ºä¸”æ˜¯åˆ—è¡¨ç±»å‹ï¼‰
                if isinstance(field_value, list) and len(field_value) > 0:
                    result_data = field_value
                    logging.info(f"æ‰¾åˆ°æœ‰æ•ˆæ•°æ®å­—æ®µ: {field}ï¼ŒåŒ…å« {len(result_data)} æ¡è®°å½•")
                    print(f"[DEBUG] æ‰¾åˆ°æœ‰æ•ˆæ•°æ®å­—æ®µ: {field}ï¼ŒåŒ…å« {len(result_data)} æ¡è®°å½•")
                    break
                elif isinstance(field_value, list) and len(field_value) == 0:
                    logging.warning(f"æ‰¾åˆ°æ•°æ®å­—æ®µ {field}ï¼Œä½†ä¸ºç©ºåˆ—è¡¨")
                    print(f"[DEBUG] æ‰¾åˆ°æ•°æ®å­—æ®µ {field}ï¼Œä½†ä¸ºç©ºåˆ—è¡¨")
                else:
                    logging.warning(f"æ‰¾åˆ°æ•°æ®å­—æ®µ {field}ï¼Œä½†ç±»å‹æˆ–æ ¼å¼ä¸æ­£ç¡®: {type(field_value)}")
                    print(f"[DEBUG] æ‰¾åˆ°æ•°æ®å­—æ®µ {field}ï¼Œä½†ç±»å‹æˆ–æ ¼å¼ä¸æ­£ç¡®: {type(field_value)}")
        
        # å¦‚æœæ²¡æ‰¾åˆ°æ ‡å‡†å­—æ®µï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯æ¿å—æ•°æ®ï¼ˆå¯èƒ½åŒ…å«å…¶ä»–å­—æ®µï¼‰
        if result_data is None:
            available_fields = list(data['data'].keys())
            logging.warning(f"æ•°æ®ä¸­ç¼ºå°‘æ ‡å‡†æ•°æ®å­—æ®µï¼Œå¯ç”¨å­—æ®µ: {available_fields}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¿å—æ•°æ®ï¼ˆæ¿å—æ•°æ®å¯èƒ½æ²¡æœ‰trendså­—æ®µï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
            # æ¿å—æ•°æ®çš„dataå­—æ®µå¯èƒ½åŒ…å«ï¼šcode, market, type, status, nameç­‰å…ƒæ•°æ®
            # ä½†å®é™…çš„Kçº¿æ•°æ®å¯èƒ½åœ¨trendsæˆ–klineså­—æ®µä¸­ï¼Œæˆ–è€…éœ€è¦ä»å…¶ä»–åœ°æ–¹è·å–
            if 'code' in data['data'] and 'market' in data['data']:
                # è¿™å¯èƒ½æ˜¯æ¿å—æ•°æ®ï¼Œä½†trendså­—æ®µå¯èƒ½ä¸ºç©ºæˆ–ä¸å­˜åœ¨
                logging.warning(f"æ£€æµ‹åˆ°æ¿å—æ•°æ®ï¼ˆcode={data['data'].get('code')}, market={data['data'].get('market')}ï¼‰ï¼Œä½†æœªæ‰¾åˆ°trends/klinesæ•°æ®")
                # å°è¯•æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å¯èƒ½åŒ…å«æ•°æ®çš„å­—æ®µ
                for key in ['trends', 'klines', 'data', 'list']:
                    if key in data['data'] and isinstance(data['data'][key], list) and len(data['data'][key]) > 0:
                        result_data = data['data'][key]
                        logging.info(f"åœ¨æ¿å—æ•°æ®ä¸­æ‰¾åˆ°æ•°æ®å­—æ®µ: {key}")
                        break
            
            if result_data is None:
                logging.error(f"æ— æ³•ä»æ•°æ®ä¸­æå–trends/klinesæ•°æ®ï¼Œå¯ç”¨å­—æ®µ: {available_fields}")
                return []
            
        logging.info(f"æˆåŠŸæå–æ•°æ®ï¼ŒåŒ…å« {len(result_data)} æ¡è®°å½•")
        return result_data

    except Exception as e:
        logging.error(f"è§£æJSONè¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
        return []


def process_data(df: pd.DataFrame, multiple: bool) -> pd.DataFrame:
    """
    å¤„ç†å¹¶æ¸…ç†æ•°æ®ã€‚

    å‚æ•°:
        df (pd.DataFrame): åŸå§‹æ•°æ®æ¡†ã€‚
        multiple (bool): æ˜¯å¦å°†09:30æ•°æ®åˆå¹¶ä¸º09:31æ•°æ®ã€‚

    è¿”å›:
        pd.DataFrame: æ¸…ç†åçš„æ•°æ®æ¡†ã€‚
    """
    if df.empty:
        return df

    # è½¬æ¢æ•°æ®ç±»å‹
    df['date'] = pd.to_datetime(df['date'])
    df[['open', 'close', 'high', 'low', 'volume', 'money']] = df[
        ['open', 'close', 'high', 'low', 'volume', 'money']].astype(float)
    df['volume'] = (df['volume'] * 100).astype('int64')
    df[['volume', 'money']] = df[['volume', 'money']].astype('int64')

    if multiple:
        # æ·»åŠ æ—¥æœŸéƒ¨åˆ†ç”¨äºåˆ†ç»„
        df['day'] = df['date'].dt.date
        df['time'] = df['date'].dt.time

        # åˆå¹¶09:30æ•°æ®åˆ°09:31
        df.loc[df['time'] == pd.Timestamp('09:30:00').time(), 'time'] = pd.Timestamp('09:31:00').time()

        # æŒ‰æ—¥æœŸå’Œæ—¶é—´è¿›è¡Œåˆ†ç»„èšåˆ
        grouped = df.groupby(['day', 'time']).agg({
            'date': 'last',
            'open': 'first',
            'close': 'last',
            'high': 'max',
            'low': 'min',
            'volume': 'sum',
            'money': 'sum'
        }).reset_index(drop=False)

        grouped = grouped.drop(columns=['day', 'time'])

        return grouped

    else:
        # å¤„ç†å•æ—¥æ•°æ®çš„æƒ…å†µ
        # åˆå¹¶09:30 æ•°æ® åˆ°09:31
        if df.iloc[0]['date'].time() == pd.Timestamp('09:30:00').time():

            df.loc[1, ['volume', 'money']] += df.loc[0, ['volume', 'money']]

            df = df.iloc[1:].reset_index(drop=True)

        return df


def get_1m_data(source: str, match: bool = False, multiple: bool = False) -> pd.DataFrame:
    """
    è·å–1åˆ†é’Ÿæ•°æ®å¹¶è¿›è¡Œæ¸…ç†ã€‚

    å‚æ•°:
        source (str): åŸå§‹æ•°æ®æºã€‚
        match (bool): æ˜¯å¦è¿›è¡Œæ­£åˆ™åŒ¹é…ã€‚
        multiple (bool): æ˜¯å¦å°†09:30æ•°æ®åˆå¹¶ä¸º09:31æ•°æ®ã€‚

    è¿”å›:
        pd.DataFrame: æ¸…ç†åçš„æ•°æ®æ¡†ã€‚å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¿”å›ç©ºçš„DataFrameã€‚
    """
    try:
        # æ·»åŠ æºæ•°æ®éªŒè¯
        if not source or not isinstance(source, str):
            logging.error(f"æ— æ•ˆçš„æºæ•°æ®æ ¼å¼: {type(source)}")
            return pd.DataFrame()
            
        logging.info(f"å¼€å§‹è§£ææ•°æ®ï¼Œæ•°æ®é•¿åº¦: {len(source)}")
        logging.debug(f"æ•°æ®æ ·æœ¬: {source[:200]}...")  # è®°å½•æ•°æ®æ ·æœ¬

        # è§£æJSONæ•°æ®
        print(f"[DEBUG] å¼€å§‹è§£æJSONæ•°æ®ï¼Œmatch={match}")
        trends = parse_json(source, match)
        print(f"[DEBUG] JSONè§£æç»“æœ: trendsæ•°é‡={len(trends) if trends else 0}")
        
        if not trends:
            print(f"[DEBUG] è§£æJSONæ•°æ®å¤±è´¥ï¼Œæœªè·å–åˆ°trendsæ•°æ®")
            logging.error("è§£æJSONæ•°æ®å¤±è´¥ï¼Œæœªè·å–åˆ°trendsæ•°æ®")
            return pd.DataFrame()

        logging.info(f"æˆåŠŸè§£æJSONæ•°æ®ï¼Œè·å–åˆ° {len(trends)} æ¡è®°å½•")
        print(f"[DEBUG] æˆåŠŸè§£æJSONæ•°æ®ï¼Œè·å–åˆ° {len(trends)} æ¡è®°å½•")

        # åˆ›å»ºDataFrame
        try:
            # æ£€æŸ¥ç¬¬ä¸€æ¡è®°å½•æœ‰å¤šå°‘åˆ—ï¼Œä»¥ç¡®å®šæ­£ç¡®çš„åˆ—æ•°
            if trends and len(trends) > 0:
                first_row_cols = len(trends[0].split(','))
                print(f"[DEBUG] ç¬¬ä¸€æ¡è®°å½•åˆ—æ•°: {first_row_cols}")
                logging.info(f"æ£€æµ‹åˆ°æ•°æ®åˆ—æ•°: {first_row_cols}")
                
                # æ ¹æ®åˆ—æ•°å®šä¹‰åˆ—å
                if first_row_cols == 7:
                    columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'money']
                elif first_row_cols == 8:
                    # æ¿å—æ•°æ®å¯èƒ½æœ‰8åˆ—ï¼Œæœ€åä¸€åˆ—å¯èƒ½æ˜¯å‡ä»·æˆ–å…¶ä»–å­—æ®µ
                    columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'money', 'avg_price']
                else:
                    logging.warning(f"æ„å¤–çš„åˆ—æ•°: {first_row_cols}ï¼Œä½¿ç”¨é»˜è®¤åˆ—å")
                    columns = [f'col_{i+1}' for i in range(first_row_cols)]
            else:
                columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'money']
            
            df = pd.DataFrame([x.split(',') for x in trends], columns=columns)
            logging.info(f"æˆåŠŸåˆ›å»ºDataFrameï¼Œå½¢çŠ¶: {df.shape}ï¼Œåˆ—: {df.columns.tolist()}")
            print(f"[DEBUG] DataFrameåˆ—: {df.columns.tolist()}")
        except Exception as e:
            logging.error(f"åˆ›å»ºDataFrameå¤±è´¥: {e}")
            print(f"[DEBUG] åˆ›å»ºDataFrameå¼‚å¸¸: {e}")
            # å°è¯•æ˜¾ç¤ºç¬¬ä¸€æ¡è®°å½•ä»¥ä¾¿è°ƒè¯•
            if trends and len(trends) > 0:
                print(f"[DEBUG] ç¬¬ä¸€æ¡è®°å½•: {trends[0]}")
                print(f"[DEBUG] ç¬¬ä¸€æ¡è®°å½•åˆ†å‰²å: {trends[0].split(',')}")
            return pd.DataFrame()

        # å¤„ç†æ•°æ®
        try:
            data = process_data(df, multiple)
            if not data.empty:
                logging.info(f"æ•°æ®å¤„ç†æˆåŠŸï¼Œæœ€ç»ˆæ•°æ®å½¢çŠ¶: {data.shape}")
                logging.debug(f"æ•°æ®æ ·æœ¬:\n{data.head()}")
            return data
        except Exception as e:
            logging.error(f"æ•°æ®å¤„ç†å¤±è´¥: {e}")
            return pd.DataFrame()

    except Exception as e:
        logging.error(f"get_1m_dataå¤„ç†è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
        return pd.DataFrame()

class HeaderRotator:
    """è¯·æ±‚å¤´è½®æ¢å™¨ - ç®¡ç†å¤šä¸ªè¯·æ±‚å¤´çš„è½®æ¢"""
    
    def __init__(self, header_type: str = 'stock_1m_multiple_days'):
        from config import Config
        self.headers_pool = Config.get_eastmoney_headers_pool(header_type)
        self.current_index = 0
        self.failed_indices = set()  # è®°å½•å¤±è´¥çš„è¯·æ±‚å¤´ç´¢å¼•
        logging.info(f"åˆå§‹åŒ–è¯·æ±‚å¤´æ± ï¼Œå…± {len(self.headers_pool)} ä¸ªè¯·æ±‚å¤´")
    
    def get_next_header(self):
        """è·å–ä¸‹ä¸€ä¸ªè¯·æ±‚å¤´"""
        if len(self.failed_indices) >= len(self.headers_pool):
            # æ‰€æœ‰è¯·æ±‚å¤´éƒ½å¤±è´¥äº†ï¼Œé‡ç½®å¤±è´¥è®°å½•
            logging.warning("æ‰€æœ‰è¯·æ±‚å¤´éƒ½å·²å¤±è´¥ï¼Œé‡ç½®è¯·æ±‚å¤´æ± ")
            self.failed_indices.clear()
            self.current_index = 0
        
        # è·³è¿‡å·²å¤±è´¥çš„è¯·æ±‚å¤´
        attempts = 0
        while self.current_index in self.failed_indices and attempts < len(self.headers_pool):
            self.current_index = (self.current_index + 1) % len(self.headers_pool)
            attempts += 1
        
        header = self.headers_pool[self.current_index].copy()
        user_agent = header.get('User-Agent', 'Unknown')
        logging.info(f"ä½¿ç”¨è¯·æ±‚å¤´ #{self.current_index + 1}/{len(self.headers_pool)}: {user_agent[:50]}...")
        return header
    
    def mark_current_failed(self):
        """æ ‡è®°å½“å‰è¯·æ±‚å¤´å¤±è´¥"""
        self.failed_indices.add(self.current_index)
        logging.warning(f"æ ‡è®°è¯·æ±‚å¤´ #{self.current_index + 1} ä¸ºå¤±è´¥çŠ¶æ€")
    
    def rotate(self):
        """è½®æ¢åˆ°ä¸‹ä¸€ä¸ªè¯·æ±‚å¤´"""
        self.current_index = (self.current_index + 1) % len(self.headers_pool)
        logging.info(f"è½®æ¢åˆ°ä¸‹ä¸€ä¸ªè¯·æ±‚å¤´ #{self.current_index + 1}")
    
    def get_current_header(self):
        """è·å–å½“å‰è¯·æ±‚å¤´"""
        return self.headers_pool[self.current_index].copy()
    
    def reset(self):
        """é‡ç½®è½®æ¢å™¨çŠ¶æ€"""
        self.current_index = 0
        self.failed_indices.clear()
        logging.info("é‡ç½®è¯·æ±‚å¤´è½®æ¢å™¨")


class DownloadData:
    """
    ä»ä¸œæ–¹è´¢å¯Œä¸‹è½½æ•°æ®
    """

    MAX_RETRIES = 3
    RETRY_DELAY = 2  # seconds
    
    # ç±»çº§åˆ«çš„è¯·æ±‚å¤´è½®æ¢å™¨ï¼ˆæŒ‰ç±»å‹å­˜å‚¨ï¼‰
    _header_rotators = {}
    
    @classmethod
    def get_header_rotator(cls, header_type: str = 'stock_1m_multiple_days'):
        """è·å–æˆ–åˆ›å»ºè¯·æ±‚å¤´è½®æ¢å™¨"""
        if header_type not in cls._header_rotators:
            cls._header_rotators[header_type] = HeaderRotator(header_type)
        return cls._header_rotators[header_type]

    @classmethod
    def validate_data(cls, df: pd.DataFrame) -> bool:
        """
        éªŒè¯ä¸‹è½½çš„æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        
        å‚æ•°:
            df (pd.DataFrame): å¾…éªŒè¯çš„æ•°æ®æ¡†
            
        è¿”å›:
            bool: æ•°æ®æ˜¯å¦æœ‰æ•ˆ
        """
        if df.empty:
            logging.error("æ•°æ®ä¸ºç©º")
            return False
            
        # æ£€æŸ¥å¿…éœ€åˆ—
        missing_cols = set(DataValidationConfig.REQUIRED_COLUMNS) - set(df.columns)
        if missing_cols:
            logging.error(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
            return False
            
        # æ£€æŸ¥æ•°æ®è¡Œæ•°
        if len(df) < DataValidationConfig.MIN_ROWS:
            logging.error(f"æ•°æ®è¡Œæ•°ä¸è¶³: {len(df)}")
            return False
            
        # æ£€æŸ¥æ—¶é—´èŒƒå›´
        if 'date' in df.columns:
            times = pd.to_datetime(df['date']).dt.time
            valid_times = times.between(
                DataValidationConfig.TRADING_START_TIME,
                DataValidationConfig.TRADING_END_TIME
            )
            if not valid_times.all():
                logging.error("å­˜åœ¨äº¤æ˜“æ—¶é—´èŒƒå›´å¤–çš„æ•°æ®")
                return False
                
        return True

    @classmethod
    def stock_1m_1day(cls, code: str) -> Optional[pd.DataFrame]:
        """
        ä»ä¸œæ–¹è´¢å¯Œç½‘ä¸‹è½½ 1 day , 1åˆ†é’Ÿè‚¡ç¥¨æ•°æ®ã€‚

        å‚æ•°:
            code (str): è‚¡ç¥¨ä»£ç ã€‚

        è¿”å›:
            Optional[pd.DataFrame]: ä¸‹è½½çš„è‚¡ç¥¨æ•°æ®ã€‚å¦‚æœä¸‹è½½å¤±è´¥ï¼Œè¿”å›Noneã€‚
        """
        import requests
        import json
        import re
        import time
        import pandas as pd
        from typing import Optional
        
        def get_stock_data(code: str) -> Optional[pd.DataFrame]:
            try:
                # è®¾ç½®è¯·æ±‚å¤´
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                    'Accept': '*/*',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive',
                    'Referer': 'http://quote.eastmoney.com/center/gridlist.html',
                }

                # æ„å»ºURLå’Œå‚æ•°
                timestamp = int(time.time() * 1000)
                params = {
                    'cb': f'jQuery112409537552066923317_{timestamp}',
                    'secid': f'0.{code}',
                    'ut': 'fa5fd1943c7b386f172d6893dbfba10b',
                    'fields1': 'f1,f2,f3,f4,f5,f6',
                    'fields2': 'f51,f52,f53,f54,f55,f56,f57,f58,f59,f60,f61',
                    'klt': '1',
                    'fqt': '1',
                    'end': '20500101',
                    'lmt': '1000',
                    '_': timestamp
                }

                # å‘é€è¯·æ±‚
                url = "http://83.push2.eastmoney.com/api/qt/stock/kline/get"
                response = requests.get(url, params=params, headers=headers, timeout=15)
                response.raise_for_status()

                # æå–JSONæ•°æ®
                text = response.text
                logging.debug(f"åŸå§‹å“åº”æ•°æ®: {text[:200]}")
                json_str = re.search(r'jQuery\d+_\d+\((.*?)\)', text)
                if not json_str:
                    logging.error("æ— æ³•æå–JSONæ•°æ®")
                    return None

                # è§£æJSON
                data = json.loads(json_str.group(1))
                if not data.get('data', {}).get('klines'):
                    logging.error("æ•°æ®ç»“æ„ä¸­æ²¡æœ‰klinesæ•°æ®")
                    return None

                # å¤„ç†æ•°æ®
                klines = data['data']['klines']
                logging.info(f"è·å–åˆ° {len(klines)} æ¡Kçº¿æ•°æ®")
                
                # å°†æ•°æ®åˆ†å‰²æˆåˆ—
                df_data = [item.split(',') for item in klines]
                if not df_data:
                    logging.error("åˆ†å‰²æ•°æ®åä¸ºç©º")
                    return None
                    
                # æ£€æŸ¥æ•°æ®åˆ—æ•°
                columns_count = len(df_data[0])
                logging.info(f"æ•°æ®åˆ—æ•°: {columns_count}")
                
                # åˆ›å»ºDataFrame
                df = pd.DataFrame(df_data)
                
                # æ ¹æ®å®é™…åˆ—æ•°è®¾ç½®åˆ—å
                all_columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'money', 
                             'amplitude', 'change_percent', 'change_amount', 'turnover']
                df.columns = all_columns[:columns_count]
                
                logging.info(f"åˆ—å: {df.columns.tolist()}")
                
                # è½¬æ¢æ•°æ®ç±»å‹
                try:
                    df['date'] = pd.to_datetime(df['date'])
                    numeric_columns = ['open', 'close', 'high', 'low']
                    for col in numeric_columns:
                        if col in df.columns:
                            df[col] = pd.to_numeric(df[col], errors='coerce')
                    
                    if 'volume' in df.columns:
                        df['volume'] = pd.to_numeric(df['volume'], errors='coerce') * 100
                    if 'money' in df.columns:
                        df['money'] = pd.to_numeric(df['money'], errors='coerce')
                    
                    # è½¬æ¢å…¶ä»–å¯èƒ½çš„æ•°å€¼åˆ—
                    optional_numeric = ['amplitude', 'change_percent', 'change_amount', 'turnover']
                    for col in optional_numeric:
                        if col in df.columns:
                            df[col] = pd.to_numeric(df[col], errors='coerce')
                            
                    logging.info("æ•°æ®ç±»å‹è½¬æ¢æˆåŠŸ")
                    
                except Exception as e:
                    logging.error(f"æ•°æ®ç±»å‹è½¬æ¢å¤±è´¥: {str(e)}")
                    return None
                
                # ç¡®ä¿è‡³å°‘åŒ…å«å¿…è¦çš„åˆ—
                required_columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'money']
                if not all(col in df.columns for col in required_columns):
                    logging.error(f"ç¼ºå°‘å¿…è¦çš„åˆ—ï¼Œå½“å‰åˆ—: {df.columns.tolist()}")
                    return None
                
                # åªè¿”å›å¿…è¦çš„åˆ—
                return df[required_columns]

            except Exception as e:
                logging.error(f"è·å–æ•°æ®å¤±è´¥: {str(e)}")
                return None

        # ä¸»å¤„ç†é€»è¾‘
        for attempt in range(cls.MAX_RETRIES):
            try:
                logging.info(f"å¼€å§‹ä¸‹è½½è‚¡ç¥¨ {code} çš„1åˆ†é’Ÿæ•°æ® (å°è¯• {attempt + 1}/{cls.MAX_RETRIES})")
                
                df = get_stock_data(code)
                if df is not None and not df.empty:
                    logging.info(f"æˆåŠŸä¸‹è½½å¹¶è§£æ {code} çš„1åˆ†é’Ÿæ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
                    show_download('1m', code)
                    return df
                
                logging.error(f"è·å–æ•°æ®å¤±è´¥æˆ–æ•°æ®ä¸ºç©º: {code}")
                if attempt < cls.MAX_RETRIES - 1:
                    time.sleep(cls.RETRY_DELAY)
                    
            except Exception as e:
                logging.error(f"å¤„ç†è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸ {code}: {str(e)}")
                if attempt < cls.MAX_RETRIES - 1:
                    time.sleep(cls.RETRY_DELAY)
                continue

        logging.error(f"åœ¨ {cls.MAX_RETRIES} æ¬¡å°è¯•åä»ç„¶å¤±è´¥: {code}")
        return None

    @classmethod
    def _get_source_with_selenium(cls, url: str) -> str:
        """
        ä½¿ç”¨ Selenium è·å–é¡µé¢æºä»£ç ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
        
        å‚æ•°:
            url (str): è¯·æ±‚çš„URL
            
        è¿”å›:
            str: é¡µé¢æºä»£ç 
        """
        driver = None
        try:
            print("[DEBUG] å¼€å§‹å¯¼å…¥ Selenium æ¨¡å—...")
            logging.info("ğŸ”§ å¼€å§‹åˆå§‹åŒ– Selenium...")
            
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            from selenium.webdriver.chrome.service import Service
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from selenium.webdriver.common.by import By
            import time
            import random
            
            print("[DEBUG] Selenium æ¨¡å—å¯¼å…¥æˆåŠŸ")
            logging.info("âœ“ Selenium æ¨¡å—å¯¼å…¥æˆåŠŸ")
            
            # ä»é…ç½®è¯»å–è®¾ç½®
            from config import Config
            use_headless = Config.DOWNLOAD_CONFIG.get('selenium_headless', True)
            
            # é…ç½® Chrome é€‰é¡¹
            chrome_options = Options()
            if use_headless:
                chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
                chrome_options.add_argument('--headless=new')  # æ–°ç‰ˆ Chrome çš„æ— å¤´æ¨¡å¼
                logging.info("Selenium ä½¿ç”¨æ— å¤´æ¨¡å¼")
                print("[DEBUG] Selenium ä½¿ç”¨æ— å¤´æ¨¡å¼")
            else:
                logging.info("Selenium ä½¿ç”¨æœ‰å¤´æ¨¡å¼ï¼ˆå¯è§æµè§ˆå™¨çª—å£ï¼‰")
                print("[DEBUG] Selenium ä½¿ç”¨æœ‰å¤´æ¨¡å¼")
            
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_argument('--window-size=1920,1080')  # è®¾ç½®çª—å£å¤§å°
            chrome_options.add_argument('--ignore-certificate-errors')  # å¿½ç•¥è¯ä¹¦é”™è¯¯
            chrome_options.add_argument('--disable-extensions')  # ç¦ç”¨æ‰©å±•
            
            print("[DEBUG] Chrome é€‰é¡¹é…ç½®å®Œæˆ")
            
            # éšæœºé€‰æ‹©ä¸€ä¸ª User-Agent
            user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15'
            ]
            chrome_options.add_argument(f'user-agent={random.choice(user_agents)}')
            
            # æ·»åŠ æ›´å¤šåæ£€æµ‹æªæ–½
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            print("[DEBUG] æ­£åœ¨åˆ›å»º Chrome é©±åŠ¨...")
            logging.info("ğŸš€ æ­£åœ¨å¯åŠ¨ Chrome æµè§ˆå™¨...")
            
            # åˆ›å»ºé©±åŠ¨ï¼ˆä¼˜å…ˆä½¿ç”¨ webdriver-manager è‡ªåŠ¨ç®¡ç†ç‰ˆæœ¬ï¼‰
            try:
                # æ–¹æ³•1: å°è¯•ä½¿ç”¨ webdriver-manager è‡ªåŠ¨ç®¡ç† ChromeDriver
                try:
                    from selenium.webdriver.chrome.service import Service
                    from webdriver_manager.chrome import ChromeDriverManager
                    
                    print("[DEBUG] ä½¿ç”¨ webdriver-manager è‡ªåŠ¨ç®¡ç† ChromeDriver ç‰ˆæœ¬...")
                    logging.info("ğŸ“¦ ä½¿ç”¨ webdriver-manager è‡ªåŠ¨ä¸‹è½½åŒ¹é…çš„ ChromeDriver...")
                    
                    service = Service(ChromeDriverManager().install())
                    driver = webdriver.Chrome(service=service, options=chrome_options)
                    
                    print("[DEBUG] Chrome é©±åŠ¨åˆ›å»ºæˆåŠŸï¼ˆä½¿ç”¨ webdriver-managerï¼‰")
                    logging.info("âœ“ Chrome æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼ˆè‡ªåŠ¨ç‰ˆæœ¬ç®¡ç†ï¼‰")
                    
                except ImportError:
                    # æ–¹æ³•2: å¦‚æœæ²¡æœ‰å®‰è£… webdriver-managerï¼Œä½¿ç”¨ç³»ç»Ÿ ChromeDriver
                    print("[DEBUG] webdriver-manager æœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨ç³»ç»Ÿ ChromeDriver...")
                    logging.warning("âš ï¸ webdriver-manager æœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨ç³»ç»Ÿ ChromeDriver")
                    logging.warning("ğŸ’¡ å»ºè®®å®‰è£…: pip install webdriver-manager")
                    
                    driver = webdriver.Chrome(options=chrome_options)
                    print("[DEBUG] Chrome é©±åŠ¨åˆ›å»ºæˆåŠŸï¼ˆä½¿ç”¨ç³»ç»Ÿ ChromeDriverï¼‰")
                    logging.info("âœ“ Chrome æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼ˆç³»ç»Ÿ ChromeDriverï¼‰")
                    
            except Exception as driver_error:
                print(f"[DEBUG] Chrome é©±åŠ¨åˆ›å»ºå¤±è´¥: {driver_error}")
                logging.error(f"âŒ Chrome é©±åŠ¨åˆ›å»ºå¤±è´¥: {driver_error}")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰ˆæœ¬ä¸åŒ¹é…é—®é¢˜
                error_str = str(driver_error)
                if "version" in error_str.lower() and "chrome" in error_str.lower():
                    logging.error("=" * 60)
                    logging.error("ğŸ”§ ChromeDriver ç‰ˆæœ¬ä¸åŒ¹é…ï¼")
                    logging.error("=" * 60)
                    logging.error("è§£å†³æ–¹æ¡ˆï¼ˆé€‰æ‹©å…¶ä¸€ï¼‰ï¼š")
                    logging.error("")
                    logging.error("æ–¹æ¡ˆ 1 - è‡ªåŠ¨ç®¡ç†ï¼ˆæ¨èï¼‰:")
                    logging.error("  pip install webdriver-manager")
                    logging.error("  é‡å¯ç¨‹åºå³å¯è‡ªåŠ¨ä¸‹è½½åŒ¹é…çš„ ChromeDriver")
                    logging.error("")
                    logging.error("æ–¹æ¡ˆ 2 - æ‰‹åŠ¨ä¸‹è½½:")
                    logging.error("  1. æŸ¥çœ‹ Chrome ç‰ˆæœ¬: chrome://version")
                    logging.error("  2. ä¸‹è½½å¯¹åº”ç‰ˆæœ¬ ChromeDriver:")
                    logging.error("     https://googlechromelabs.github.io/chrome-for-testing/")
                    logging.error("  3. æ›¿æ¢æ—§çš„ chromedriver.exe")
                    logging.error("=" * 60)
                else:
                    logging.error("è¯·æ£€æŸ¥ï¼š")
                    logging.error("  1. æ˜¯å¦å·²å®‰è£… Chrome æµè§ˆå™¨")
                    logging.error("  2. æ˜¯å¦å·²å®‰è£… ChromeDriver")
                    logging.error("  3. ChromeDriver ç‰ˆæœ¬æ˜¯å¦ä¸ Chrome ç‰ˆæœ¬åŒ¹é…")
                    logging.error("  4. ChromeDriver æ˜¯å¦åœ¨ç³»ç»Ÿ PATH ä¸­")
                
                raise
            
            # è®¾ç½®é¡µé¢åŠ è½½è¶…æ—¶
            driver.set_page_load_timeout(30)
            driver.set_script_timeout(30)
            print("[DEBUG] è®¾ç½®è¶…æ—¶æ—¶é—´å®Œæˆ")
            
            # æ‰§è¡Œåæ£€æµ‹è„šæœ¬
            driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                '''
            })
            
            print(f"[DEBUG] Selenium è®¿é—® URL: {url}")
            logging.info(f"ğŸŒ Selenium è®¿é—® URL: {url}")
            
            try:
                driver.get(url)
                print("[DEBUG] é¡µé¢åŠ è½½å®Œæˆ")
                logging.info("âœ“ é¡µé¢åŠ è½½å®Œæˆ")
            except Exception as load_error:
                print(f"[DEBUG] é¡µé¢åŠ è½½å¤±è´¥: {load_error}")
                logging.error(f"âŒ é¡µé¢åŠ è½½å¤±è´¥: {load_error}")
                raise
            
            # éšæœºç­‰å¾…ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
            wait_time = random.uniform(3, 6)
            print(f"[DEBUG] ç­‰å¾… {wait_time:.1f} ç§’åŠ è½½é¡µé¢...")
            logging.info(f"â³ ç­‰å¾… {wait_time:.1f} ç§’åŠ è½½é¡µé¢...")
            time.sleep(wait_time)
            
            # è·å–é¡µé¢æºç 
            print("[DEBUG] è·å–é¡µé¢æºç ...")
            page_source = driver.page_source
            print(f"[DEBUG] è·å–åˆ°é¡µé¢æºç ï¼Œé•¿åº¦: {len(page_source) if page_source else 0}")
            print(f"[DEBUG] é¡µé¢æºç å‰200å­—ç¬¦: {page_source[:200] if page_source else 'None'}")
            
            if not page_source or len(page_source) < 100:
                logging.error("âŒ Selenium è·å–çš„é¡µé¢æºç ä¸ºç©ºæˆ–è¿‡çŸ­")
                print("[DEBUG] é¡µé¢æºç ä¸ºç©ºæˆ–è¿‡çŸ­")
                return ""
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ HTML é”™è¯¯é¡µé¢ï¼ˆåº”è¯¥æ˜¯ JSON æˆ– JSONPï¼‰
            if page_source.strip().startswith('<'):
                logging.error("âŒ Selenium è¿”å›çš„æ˜¯ HTML é¡µé¢ï¼Œè€Œä¸æ˜¯ JSON æ•°æ®")
                logging.error("è¯´æ˜ï¼šä¸œæ–¹è´¢å¯Œç½‘æ£€æµ‹åˆ°äº†è‡ªåŠ¨åŒ–è®¿é—®")
                print("[DEBUG] è¿”å›çš„æ˜¯ HTML é¡µé¢ï¼Œä¸æ˜¯ JSON æ•°æ®")
                print(f"[DEBUG] HTML å†…å®¹å‰500å­—ç¬¦: {page_source[:500]}")
                
                # å°è¯•ä» HTML ä¸­æå–é”™è¯¯ä¿¡æ¯
                if "è®¿é—®å¼‚å¸¸" in page_source or "Access Denied" in page_source:
                    logging.error("âŒ è®¿é—®è¢«æ‹’ç»ï¼Œå¯èƒ½æ˜¯åçˆ¬è™«æœºåˆ¶")
                
                return ""
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é¢„æœŸçš„æ•°æ®
            if 'jQuery' in page_source or '{' in page_source:
                logging.info(f"âœ“ Selenium æˆåŠŸè·å–é¡µé¢æºç ï¼Œé•¿åº¦: {len(page_source)}")
                print(f"[DEBUG] é¡µé¢å†…å®¹çœ‹èµ·æ¥æ˜¯ JSON/JSONP æ•°æ®")
                return page_source
            else:
                logging.error("âŒ é¡µé¢å†…å®¹æ ¼å¼ä¸æ­£ç¡®")
                print("[DEBUG] é¡µé¢å†…å®¹æ ¼å¼ä¸æ­£ç¡®")
                return ""
        
        except ImportError as ie:
            print(f"[DEBUG] å¯¼å…¥é”™è¯¯: {ie}")
            logging.error(f"âŒ Selenium æ¨¡å—å¯¼å…¥å¤±è´¥: {str(ie)}")
            logging.error("è¯·å®‰è£… Selenium: pip install selenium")
            return ""
                
        except Exception as e:
            print(f"[DEBUG] Selenium å¼‚å¸¸: {type(e).__name__}: {str(e)}")
            logging.error(f"âŒ Selenium è·å–æ•°æ®å¤±è´¥: {str(e)}")
            import traceback
            error_trace = traceback.format_exc()
            print(f"[DEBUG] å¼‚å¸¸å †æ ˆ:\n{error_trace}")
            logging.error(f"å¼‚å¸¸å †æ ˆ: {error_trace}")
            
            # æä¾›æ›´å…·ä½“çš„é”™è¯¯æç¤º
            if "chromedriver" in str(e).lower() or "chrome" in str(e).lower():
                logging.error("=" * 60)
                logging.error("ChromeDriver ç›¸å…³é”™è¯¯ï¼Œè¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š")
                logging.error("1. å®‰è£… ChromeDriver:")
                logging.error("   - ä¸‹è½½åœ°å€: https://chromedriver.chromium.org/downloads")
                logging.error("   - æˆ–ä½¿ç”¨: pip install webdriver-manager")
                logging.error("2. ç¡®ä¿ Chrome æµè§ˆå™¨å·²å®‰è£…")
                logging.error("3. ç¡®ä¿ ChromeDriver ç‰ˆæœ¬ä¸ Chrome ç‰ˆæœ¬åŒ¹é…")
                logging.error("4. å°† ChromeDriver æ·»åŠ åˆ°ç³»ç»Ÿ PATH")
                logging.error("=" * 60)
            
            return ""
        finally:
            # ç¡®ä¿å…³é—­æµè§ˆå™¨
            if driver:
                try:
                    driver.quit()
                    logging.info("Selenium æµè§ˆå™¨å·²å…³é—­")
                except:
                    pass
    
    @classmethod
    def _get_source_with_rotation(cls, url: str, header_type: str = 'stock_1m_multiple_days', use_selenium_fallback: bool = None) -> str:
        """
        ä½¿ç”¨è¯·æ±‚å¤´è·å–é¡µé¢æºä»£ç ï¼Œå¤±è´¥æ—¶ç«‹å³é™çº§åˆ° Selenium
        
        å‚æ•°:
            url (str): è¯·æ±‚çš„URL
            header_type (str): è¯·æ±‚å¤´ç±»å‹
            use_selenium_fallback (bool): æ˜¯å¦å¯ç”¨ Selenium é™çº§ï¼ŒNoneåˆ™ä»é…ç½®è¯»å–
            
        è¿”å›:
            str: é¡µé¢æºä»£ç 
        """
        # ä»é…ç½®è¯»å–æ˜¯å¦å¯ç”¨ Selenium é™çº§
        if use_selenium_fallback is None:
            from config import Config
            use_selenium_fallback = Config.DOWNLOAD_CONFIG.get('use_selenium_fallback', True)
        
        rotator = cls.get_header_rotator(header_type)
        
        # å­é˜¶æ®µ 2.1ï¼šå°è¯•1ä¸ªè¯·æ±‚å¤´
        logging.info(f"å°è¯•ä½¿ç”¨ HTTP è¯·æ±‚ï¼ˆè¯·æ±‚å¤´è½®æ¢ï¼Œç±»å‹: {header_type}ï¼‰")
        print(f"[DEBUG] ä½¿ç”¨è¯·æ±‚å¤´ç±»å‹: {header_type}")
        
        # è·å–å½“å‰è¯·æ±‚å¤´
        headers = rotator.get_next_header()
        
        try:
            content = cls._get_source(url, headers)
            if content and len(content) > 100:  # ç¡®ä¿è·å–åˆ°æœ‰æ•ˆå†…å®¹
                logging.info(f"âœ“ ä½¿ç”¨è¯·æ±‚å¤´ #{rotator.current_index + 1} æˆåŠŸè·å–æ•°æ®")
                rotator.rotate()  # æˆåŠŸåè½®æ¢åˆ°ä¸‹ä¸€ä¸ªï¼Œè®©ä¸‹æ¬¡ä½¿ç”¨ä¸åŒçš„è¯·æ±‚å¤´
                return content
            else:
                logging.warning(f"âœ— è¯·æ±‚å¤´ #{rotator.current_index + 1} è¿”å›ç©ºå†…å®¹")
        except Exception as e:
            logging.error(f"âœ— è¯·æ±‚å¤´ #{rotator.current_index + 1} è¯·æ±‚å¤±è´¥: {e}")
            rotator.mark_current_failed()
        
        logging.warning("HTTP è¯·æ±‚å¤±è´¥")
        rotator.rotate()  # å¤±è´¥åè½®æ¢åˆ°ä¸‹ä¸€ä¸ª
        
        # å­é˜¶æ®µ 2.2ï¼šç«‹å³é™çº§åˆ° Selenium
        if use_selenium_fallback:
            print("")
            print("[DEBUG] ------------------------------------------------------------")
            print("[DEBUG] é™çº§ä½¿ç”¨ Selenium æµè§ˆå™¨æ¨¡æ‹Ÿ")
            print("[DEBUG] ------------------------------------------------------------")
            logging.info("")
            logging.warning("é™çº§ä½¿ç”¨ Selenium è·å–æ•°æ®")
            
            try:
                print("[DEBUG] è°ƒç”¨ _get_source_with_selenium...")
                content = cls._get_source_with_selenium(url)
                print(f"[DEBUG] _get_source_with_selenium è¿”å›å†…å®¹é•¿åº¦: {len(content) if content else 0}")
                
                if content and len(content) > 100:
                    logging.info("âœ… Selenium æˆåŠŸè·å–æ•°æ®")
                    print("[DEBUG] Selenium æˆåŠŸè·å–æ•°æ®")
                    return content
                else:
                    logging.error("âŒ Selenium è·å–å¤±è´¥æˆ–æ•°æ®ä¸ºç©º")
                    print("[DEBUG] Selenium è¿”å›ç©ºæ•°æ®")
            except Exception as e:
                logging.error(f"âŒ Selenium æ‰§è¡Œå¼‚å¸¸: {e}")
                print(f"[DEBUG] Selenium æ‰§è¡Œå¼‚å¸¸: {e}")
                import traceback
                print(f"[DEBUG] å¼‚å¸¸è¯¦æƒ…:\n{traceback.format_exc()}")
        else:
            print("[DEBUG] Selenium é™çº§å·²ç¦ç”¨ï¼ˆuse_selenium_fallback=Falseï¼‰")
            logging.warning("âš ï¸ Selenium é™çº§å·²ç¦ç”¨")
        
        logging.error("âŒ ä¸œæ–¹è´¢å¯Œå’ŒSeleniuméƒ½å¤±è´¥")
        print("[DEBUG] ä¸œæ–¹è´¢å¯Œå’ŒSeleniuméƒ½å¤±è´¥ï¼Œè¿”å›ç©º")
        return ""
    
    @staticmethod
    def _get_source(url: str, headers: dict) -> str:
        """
        è·å–é¡µé¢æºä»£ç ã€‚

        å‚æ•°:
            url (str): è¯·æ±‚çš„URLã€‚
            headers (dict): è¯·æ±‚å¤´ä¿¡æ¯ã€‚

        è¿”å›:
            str: é¡µé¢æºä»£ç ã€‚
        """
        try:
            import requests
            from requests.adapters import HTTPAdapter
            from urllib3.util.retry import Retry
            import urllib3
            
            # ç¦ç”¨SSLè­¦å‘Š
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            
            # åˆ›å»ºsessionå¹¶è®¾ç½®é‡è¯•ç­–ç•¥
            session = requests.Session()
            
            # ä¼˜åŒ–çš„é‡è¯•ç­–ç•¥ - æ›´å¤šé‡è¯•ï¼Œæ›´é•¿å»¶è¿Ÿ
            retry_strategy = Retry(
                total=4,  # é‡è¯•4æ¬¡ï¼ˆç»™APIæ›´å¤šæœºä¼šï¼‰
                backoff_factor=2,  # å»¶è¿Ÿå› å­2ç§’ï¼ˆ2, 4, 8, 16ç§’é€’å¢ï¼‰
                status_forcelist=[500, 502, 503, 504, 429, 408],  # æ·»åŠ 408è¶…æ—¶çŠ¶æ€ç 
                allowed_methods=["GET", "HEAD"],
                # å…³é”®ï¼šå¢åŠ å¯¹è¿æ¥é”™è¯¯çš„é‡è¯•
                raise_on_status=False,
                # å¤„ç†è¿æ¥ä¸­æ–­
                respect_retry_after_header=True
            )
            
            # ä¼˜åŒ–è¿æ¥æ± é…ç½®
            adapter = HTTPAdapter(
                max_retries=retry_strategy,
                pool_connections=10,  # è¿æ¥æ± æ•°é‡
                pool_maxsize=20,      # æœ€å¤§è¿æ¥æ•°
                pool_block=False      # è¿æ¥æ± æ»¡æ—¶ä¸é˜»å¡
            )
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            
            # å‘é€è¯·æ±‚å‰çš„éšæœºå»¶è¿Ÿï¼ˆæ›´é•¿å»¶è¿Ÿï¼Œé¿å…è¢«é™æµï¼‰
            import time
            import random
            delay = random.uniform(3, 5)  # éšæœºå»¶è¿Ÿ3-5ç§’ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
            logging.info(f"è¯·æ±‚å»¶è¿Ÿ {delay:.2f} ç§’ï¼ˆé¿å…é™æµï¼‰")
            print(f"[DEBUG] è¯·æ±‚å»¶è¿Ÿ {delay:.2f} ç§’")
            time.sleep(delay)
            
            # æ›´æ–°è¯·æ±‚å¤´ï¼Œä½¿å…¶æ›´åƒçœŸå®æµè§ˆå™¨
            headers_copy = headers.copy()
            headers_copy.update({
                'Accept': '*/*',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Cache-Control': 'no-cache',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Pragma': 'no-cache',
            })
            
            # å‘é€è¯·æ±‚ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´
            print(f"[DEBUG] æ­£åœ¨å‘é€ GET è¯·æ±‚...")
            logging.info(f"ğŸ“¡ æ­£åœ¨è¯·æ±‚: {url[:80]}...")
            
            response = session.get(
                url,
                headers=headers_copy,
                timeout=(10, 20),  # è¿æ¥è¶…æ—¶10ç§’ï¼Œè¯»å–è¶…æ—¶20ç§’ï¼ˆç»™æ›´å¤šæ—¶é—´ï¼‰
                verify=False,
                allow_redirects=True,
                # å…³é”®ï¼šå¯ç”¨æŒä¹…è¿æ¥
                stream=False
            )
            
            print(f"[DEBUG] è¯·æ±‚å®Œæˆï¼ŒçŠ¶æ€ç : {response.status_code}")
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            response.raise_for_status()
            
            # è®°å½•å“åº”ä¿¡æ¯
            logging.info(f"âœ“ æˆåŠŸè·å–å“åº”ï¼ŒçŠ¶æ€ç : {response.status_code}")
            print(f"[DEBUG] å“åº”å¤´: {dict(response.headers)}")
            logging.debug(f"å“åº”å¤´: {dict(response.headers)}")
            
            # ç¡®ä¿æ­£ç¡®å¤„ç†å‹ç¼©æ•°æ®
            content_encoding = response.headers.get('Content-Encoding', '')
            logging.debug(f"å“åº”ç¼–ç : {content_encoding}")
            
            if content_encoding == 'br':
                # Brotliå‹ç¼©ï¼Œå°è¯•æ‰‹åŠ¨è§£å‹
                try:
                    import brotli
                    content = brotli.decompress(response.content).decode('utf-8')
                    logging.info(f"Brotliè§£å‹ç¼©æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)}")
                except Exception as e:
                    logging.warning(f"Brotliè§£å‹ç¼©å¤±è´¥ï¼Œä½¿ç”¨response.text: {e}")
                    content = response.text
            else:
                # å…¶ä»–å‹ç¼©æ ¼å¼ï¼Œä½¿ç”¨response.text
                content = response.text
            
            # å…³é—­session
            session.close()
            
            logging.info(f"æœ€ç»ˆå†…å®¹é•¿åº¦: {len(content)}")
            logging.debug(f"å†…å®¹å‰200å­—ç¬¦: {content[:200]}")
            
            return content

        except requests.exceptions.ConnectionError as e:
            print(f"[DEBUG] è¿æ¥é”™è¯¯: {str(e)}")
            logging.error(f"âŒ è¿æ¥é”™è¯¯ {url[:80]}...: {str(e)}")
            logging.warning("ğŸ’¡ å»ºè®®ï¼š1) æ£€æŸ¥ç½‘ç»œè¿æ¥ 2) å‡å°‘å¹¶å‘è¯·æ±‚æ•° 3) å¢åŠ è¯·æ±‚å»¶è¿Ÿ")
            return ""
        except requests.exceptions.Timeout as e:
            print(f"[DEBUG] è¯·æ±‚è¶…æ—¶: {str(e)}")
            logging.error(f"â±ï¸ è¯·æ±‚è¶…æ—¶ {url[:80]}...: {str(e)}")
            logging.warning("ğŸ’¡ å»ºè®®ï¼šå¢åŠ è¶…æ—¶æ—¶é—´æˆ–ç¨åé‡è¯•")
            return ""
        except requests.exceptions.TooManyRedirects as e:
            print(f"[DEBUG] é‡å®šå‘æ¬¡æ•°è¿‡å¤š: {str(e)}")
            logging.error(f"ğŸ”„ é‡å®šå‘æ¬¡æ•°è¿‡å¤š: {url[:80]}...")
            return ""
        except requests.exceptions.RequestException as e:
            print(f"[DEBUG] è¯·æ±‚å¼‚å¸¸: {str(e)}")
            logging.error(f"âŒ è¯·æ±‚å¼‚å¸¸ {url[:80]}...: {str(e)}")
            return ""
        except Exception as e:
            print(f"[DEBUG] æœªçŸ¥å¼‚å¸¸: {type(e).__name__}: {str(e)}")
            logging.error(f"âŒ è·å–æºæ•°æ®æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            import traceback
            error_trace = traceback.format_exc()
            print(f"[DEBUG] å¼‚å¸¸å †æ ˆ:\n{error_trace}")
            logging.error(f"å¼‚å¸¸å †æ ˆ: {error_trace}")
            return ""

    @staticmethod
    def _handle_empty_source(code: str):
        """
        å¤„ç†ç©ºæ•°æ®æºçš„æƒ…å†µå¹¶è®°å½•è­¦å‘Šä¿¡æ¯ã€‚

        å‚æ•°:
            code (str): è‚¡ç¥¨ä»£ç ã€‚
        """
        info_text = f"Failed to retrieve code_data for {code}. Source is empty."
        logging.warning(info_text)
        return pd.DataFrame()

    @classmethod
    def stock_1m_days(cls, code: str, days: int = 5) -> pd.DataFrame:
        """
        ä¸‹è½½ N days 1åˆ†é’Ÿè‚¡ç¥¨æ•°æ®ï¼ˆæ™ºèƒ½ä¸‰é˜¶æ®µç­–ç•¥ï¼‰
        
        ä¼˜å…ˆçº§é¡ºåºï¼š
        1. pytdxï¼ˆç¨³å®šå¿«é€Ÿï¼Œä¼˜å…ˆä½¿ç”¨ï¼‰
        2. ä¸œæ–¹è´¢å¯Œ APIï¼ˆè¯·æ±‚å¤´è½®æ¢ï¼‰
        3. Seleniumï¼ˆæµè§ˆå™¨æ¨¡æ‹Ÿï¼‰

        å‚æ•°:
            code (str): è‚¡ç¥¨ä»£ç ã€‚
            days (int): éœ€è¦ä¸‹è½½çš„å¤©æ•°ï¼Œé»˜è®¤ä¸º5å¤©ã€‚

        è¿”å›:
            pd.DataFrame: ä¸‹è½½çš„è‚¡ç¥¨æ•°æ®ã€‚
        """
        print(f"[DEBUG] DlEastMoney.stock_1m_days è¢«è°ƒç”¨: code={code}, days={days}")
        logging.info("*" * 80)
        logging.info(f"å¼€å§‹ä¸‹è½½è‚¡ç¥¨ {code} çš„ {days} å¤©æ•°æ®")
        logging.info("*" * 80)
        
        try:
            # ============================================================
            # ç¬¬ä¸€é˜¶æ®µï¼šä¼˜å…ˆä½¿ç”¨ pytdxï¼ˆç¨³å®šå¯é ï¼‰
            # ============================================================
            print(f"[DEBUG] ============================================================")
            print(f"[DEBUG] ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨ pytdx è·å–æ•°æ®ï¼ˆä¼˜å…ˆæ–¹æ¡ˆï¼‰")
            print(f"[DEBUG] ============================================================")
            logging.info("=" * 60)
            logging.info("ğŸš€ ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨ pytdx è·å–æ•°æ®ï¼ˆä¼˜å…ˆæ–¹æ¡ˆï¼‰")
            logging.info("=" * 60)
            
            try:
                from App.codes.downloads.DlPytdx import download_stock_1m_pytdx
                
                df_pytdx, end_date = download_stock_1m_pytdx(code, days)
                
                if not df_pytdx.empty:
                    logging.info(f"âœ… pytdx æˆåŠŸè·å– {len(df_pytdx)} æ¡æ•°æ®")
                    print(f"[DEBUG] âœ“ pytdx æˆåŠŸè·å– {len(df_pytdx)} æ¡æ•°æ®")
                    
                    show_download('1m', code)
                    logging.info("*" * 80)
                    logging.info(f"âœ… æˆåŠŸä¸‹è½½è‚¡ç¥¨ {code} çš„ {len(df_pytdx)} æ¡è®°å½•")
                    logging.info("*" * 80)
                    return df_pytdx
                else:
                    logging.warning("âš ï¸ pytdx è¿”å›ç©ºæ•°æ®ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ")
                    print(f"[DEBUG] âœ— pytdx è¿”å›ç©ºæ•°æ®")
                    
            except ImportError:
                logging.warning("âš ï¸ pytdx æœªå®‰è£…ï¼Œè·³è¿‡ç¬¬ä¸€é˜¶æ®µ")
                print(f"[DEBUG] pytdx æœªå®‰è£…ï¼Œè·³è¿‡")
            except Exception as e:
                logging.warning(f"âš ï¸ pytdx è·å–å¤±è´¥: {e}")
                print(f"[DEBUG] pytdx å¤±è´¥: {e}")
            
            # ============================================================
            # ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨ä¸œæ–¹è´¢å¯Œ API + Selenium
            # ============================================================
            print(f"")
            print(f"[DEBUG] ============================================================")
            print(f"[DEBUG] ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨ä¸œæ–¹è´¢å¯Œ APIï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰")
            print(f"[DEBUG] ============================================================")
            logging.info("")
            logging.info("=" * 60)
            logging.info("ğŸ”„ ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨ä¸œæ–¹è´¢å¯Œ APIï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰")
            logging.info("=" * 60)
            
            url = my_url('stock_1m_multiple_days').format(days, UrlCode(code))
            print(f"[DEBUG] ç”Ÿæˆçš„URL: {url}")
            
            # ä½¿ç”¨è¯·æ±‚å¤´è½®æ¢æœºåˆ¶è·å–æ•°æ®ï¼ˆå¤±è´¥æ—¶ä¼šè‡ªåŠ¨é™çº§åˆ° Seleniumï¼‰
            source = cls._get_source_with_rotation(url, 'stock_1m_multiple_days')
            print(f"[DEBUG] è·å–åˆ°çš„æºæ•°æ®é•¿åº¦: {len(source) if source else 0}")
            
            if source:
                print(f"[DEBUG] æºæ•°æ®å‰200å­—ç¬¦: {source[:200]}")
            else:
                print(f"[DEBUG] æºæ•°æ®ä¸ºç©º")

            if not source:
                logging.error(f"âŒ æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥ï¼Œæ— æ³•è·å–è‚¡ç¥¨ {code} çš„æ•°æ®")
                print(f"[DEBUG] æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥")
                return cls._handle_empty_source(code)

            # è§£æä¸œæ–¹è´¢å¯Œæ•°æ®
            print(f"[DEBUG] å¼€å§‹è§£æä¸œæ–¹è´¢å¯Œæ•°æ®...")
            logging.info(f"å¼€å§‹è§£æä¸‹è½½çš„æ•°æ®...")
            # æ³¨æ„ï¼š_get_source_with_rotation è¿”å›çš„æ˜¯çº¯JSONï¼Œä¸éœ€è¦æ­£åˆ™åŒ¹é…
            dl = get_1m_data(source, match=False, multiple=True)
            print(f"[DEBUG] æ•°æ®è§£æç»“æœ: å½¢çŠ¶={dl.shape if not dl.empty else 'Empty'}")

            if dl.empty:
                print(f"[DEBUG] è‚¡ç¥¨ {code} æ•°æ®è§£æåä¸ºç©º")
                logging.warning(f"âš ï¸ è‚¡ç¥¨ {code} æ•°æ®è§£æåä¸ºç©º")
                return dl

            show_download('1m', code)
            logging.info("*" * 80)
            logging.info(f"âœ… æˆåŠŸä¸‹è½½è‚¡ç¥¨ {code} çš„ {len(dl)} æ¡è®°å½•")
            logging.info("*" * 80)
            print(f"[DEBUG] æˆåŠŸä¸‹è½½è‚¡ç¥¨ {code} çš„ {len(dl)} æ¡è®°å½•")
            return dl
            
        except Exception as e:
            print(f"[DEBUG] ä¸‹è½½è‚¡ç¥¨ {code} æ•°æ®æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            logging.error(f"âŒ ä¸‹è½½è‚¡ç¥¨ {code} æ•°æ®æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            import traceback
            logging.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return pd.DataFrame()

    @classmethod
    def board_1m_data(cls, code: str):
        headers = my_headers('board_1m_data')
        url = my_url('board_1m_data').format(code)
        source = page_source(url=url, headers=headers)
        dl = get_1m_data(source, match=True, multiple=False)

        show_download('1m', code)  # æ‰“å°ä¸‹è½½
        return dl

    @classmethod
    def board_1m_multiple(cls, code: str, days=5):
        """
        ä¸‹è½½æ¿å—å¤šå¤©1åˆ†é’Ÿæ•°æ®
        
        å‚æ•°:
            code (str): æ¿å—ä»£ç ï¼ˆå¦‚ BK0437ï¼‰
            days (int): éœ€è¦ä¸‹è½½çš„å¤©æ•°ï¼Œé»˜è®¤ä¸º5å¤©
            
        è¿”å›:
            pd.DataFrame: ä¸‹è½½çš„æ¿å—æ•°æ®
        """
        logging.info(f"å¼€å§‹ä¸‹è½½æ¿å— {code} çš„ {days} å¤©æ•°æ®...")
        print(f"[DEBUG] å¼€å§‹ä¸‹è½½æ¿å— {code} çš„ {days} å¤©æ•°æ®...")
        
        url = my_url('board_1m_multiple_days').format(days, code)
        print(f"[DEBUG] ç”Ÿæˆçš„URL: {url}")
        logging.info(f"å°è¯•è®¿é—®URL: {url}")
        
        # ä½¿ç”¨ä¸ stock_1m_days ç›¸åŒçš„æ•°æ®è·å–æ–¹æ³•ï¼Œç¡®ä¿æ­£ç¡®å¤„ç†å‹ç¼©å“åº”
        source = cls._get_source_with_rotation(url, 'board_1m_multiple_days')
        print(f"[DEBUG] è·å–åˆ°çš„æºæ•°æ®é•¿åº¦: {len(source) if source else 0}")
        
        if source:
            print(f"[DEBUG] æºæ•°æ®å‰200å­—ç¬¦: {source[:200]}")
        else:
            print(f"[DEBUG] æºæ•°æ®ä¸ºç©º")
            logging.warning(f"Failed to retrieve data for {code}. Source is empty.")
            return pd.DataFrame()

        # è§£æä¸œæ–¹è´¢å¯Œæ•°æ®
        print(f"[DEBUG] å¼€å§‹è§£æä¸œæ–¹è´¢å¯Œæ•°æ®...")
        logging.info(f"å¼€å§‹è§£æä¸‹è½½çš„æ•°æ®...")
        dl = get_1m_data(source, match=False, multiple=True)
        print(f"[DEBUG] æ•°æ®è§£æç»“æœ: å½¢çŠ¶={dl.shape if not dl.empty else 'Empty'}")

        if dl.empty:
            print(f"[DEBUG] æ¿å— {code} æ•°æ®è§£æåä¸ºç©º")
            logging.warning(f"âš ï¸ æ¿å— {code} æ•°æ®è§£æåä¸ºç©º")
            return dl

        show_download('1m', code)
        logging.info("*" * 80)
        logging.info(f"âœ… æˆåŠŸä¸‹è½½æ¿å— {code} çš„ {len(dl)} æ¡è®°å½•")
        logging.info("*" * 80)
        print(f"[DEBUG] æˆåŠŸä¸‹è½½æ¿å— {code} çš„ {len(dl)} æ¡è®°å½•")
        return dl

    @classmethod
    def funds_to_stock(cls):

        """
        ä»ä¸œæ–¹è´¢å¯Œç½‘ä¸‹è½½åŒ—å‘èµ„é‡‘æµå…¥ä¸ªè‚¡æ•°æ®ï¼š
        ä¸‹è½½åŒ—å‘èµ„é‡‘æ¯æ—¥æµå‘æ•°æ®ï¼Œé€šè¿‡åŒ—å‘èµ„é‡‘æµå…¥ï¼Œé€‰æ‹©è‡ªå·±çš„è‚¡ç¥¨æ± ;
        åŒ—å‘èµ„é‡‘å·²ç»ä¸‹è½½æ•°æ®ï¼›
        """

        # åŒ—å‘èµ„é‡‘ä¸ªè‚¡æµå…¥ä¸ªè‚¡ç½‘å€
        page1 = 'http://data.eastmoney.com/hsgtcg/list.html'
        page2 = '/html/body/div[1]/div[8]/div[2]/div[2]/div[2]/div[3]/div[3]/div[1]/a[2]'
        page3 = '/html/body/div[1]/div[8]/div[2]/div[2]/div[2]/div[3]/div[3]/div[1]/a[4]'

        path_date = '/html/body/div[1]/div[8]/div[2]/div[2]/div[1]/div[1]/div/span'

        driver = webdriver.Chrome()
        driver.get(page1)

        new_date = driver.find_element('xpath', path_date).text[1:-1]
        new_date = pd.to_datetime(new_date)

        # è·å–ç¬¬1é¡µ50æ¡æ•°æ®
        source01 = driver.page_source
        dl01 = return_FundsData(source01, new_date)

        # è·å–ç¬¬2é¡µ50æ¡æ•°æ®
        driver.find_element('xpath', page2).click()
        time.sleep(6)
        source02 = driver.page_source
        dl02 = return_FundsData(source02, new_date)

        # è·å–ç¬¬3é¡µ50æ¡æ•°æ®
        driver.find_element('xpath', page3).click()
        time.sleep(6)
        source03 = driver.page_source
        dl03 = return_FundsData(source03, new_date)
        driver.close()

        # åˆå¹¶æ•°æ®ï¼š
        data = pd.concat([dl01, dl02, dl03], ignore_index=True).reset_index(drop=True)

        print(f'ä¸œæ–¹è´¢å¯Œä¸‹è½½{new_date}æ—¥åŒ—å‘èµ„é‡‘æµå…¥ä¸ªè‚¡æ®æˆåŠŸ;')

        return data

    @classmethod
    def funds_to_stock2(cls):
        """
        ideal:
        try to use the web link download code_data;
        # http://datacenter-web.eastmoney.com/api/data/v1/get?callback=jQuery112309232266440254648_1657933725762&sortColumns=
        ADD_MARKET_CAP&sortTypes=-1&pageSize=50&pageNumber=1&reportName=RPT_MUTUAL_STOCK_NORTHSTA&columns=
        ALL&source=WEB&client=WEB&filter=(TRADE_DATE%3D%272022-07-15%27)(INTERVAL_TYPE%3D%221%22)

        target:
        last function:
        1. page size = 200;
        2. down full code_data
        """

        page_size = 50
        # date_ = pd.to
        w1 = 'http://datacenter-web.eastmoney.com/api/data/v1/get?callback=jQuery112309232266440254648_1657933725762' \
             '&sortColumns= '
        w2 = f'ADD_MARKET_CAP&sortTypes=-1&pageSize={page_size}&pageNumber=1&reportName=RPT_MUTUAL_STOCK_NORTHSTA' \
             f'&columns= '
        w3 = 'ALL&source=WEB&client=WEB&filter=(TRADE_DATE%3D%272022-07-16%27)(INTERVAL_TYPE%3D%221%22)'
        web = f'{w1}{w2}{w3}'
        print(web)
        pass

    @classmethod
    def funds_month_history(cls):  # åŒ—å‘èµ„é‡‘è¿‘1ä¸ªæœˆæµå…¥

        headers = my_headers('funds_month_history')

        url = my_url('funds_month_history')

        source = page_source(url=url, headers=headers)

        p1 = re.compile(r'[(](.*?)[)]', re.S)  # æœ€å°åŒ¹é…
        dl = re.findall(p1, source)[0]
        dl = pd.DataFrame(data=json.loads(dl)['result']['code_data'])
        dl = dl[['TRADE_DATE', 'NET_INFLOW_SH', 'NET_INFLOW_SZ', 'NET_INFLOW_BOTH']]
        dl.loc[:, 'TRADE_DATE'] = pd.to_datetime(dl['TRADE_DATE']).dt.date
        dl = dl.rename(columns={'TRADE_DATE': 'trade_date'})
        print('ä¸œæ–¹è´¢å¯Œä¸‹è½½è¿‘ä¸€ä¸ªæœˆåŒ—å‘èµ„é‡‘æ•°æ®æˆåŠŸ;')
        return dl

    @classmethod
    def funds_daily_data(cls):
        web_01 = 'https://data.eastmoney.com/hsgt/'
        driver = webdriver.Chrome()
        driver.set_page_load_timeout(60)
        driver.set_script_timeout(60)
        driver.get(web_01)

        # æ›´æ–°æ—¶é—´ 07-30
        update_xpath = '/html/body/div[1]/div[8]/div[2]/div[2]/div[3]/div[1]/div[3]/span'

        # æ²ªè‚¡é€š å‡€æµå…¥: SH money xpathï¼Œ æ·±è‚¡é€š å‡€æµå…¥: SZ money xpath ï¼Œ åŒ—å‘ å‡€æµå…¥: North_sum_xpath
        hmx = '/html/body/div[1]/div[8]/div[2]/div[2]/div[3]/div[6]/ul[1]/li[1]/span[2]/span/span'
        zmx = '/html/body/div[1]/div[8]/div[2]/div[2]/div[3]/div[6]/ul[1]/li[2]/span[2]/span/span'
        nsx = '/html/body/div[1]/div[8]/div[2]/div[2]/div[3]/div[6]/ul[1]/li[3]/span[2]/span/span'

        update = driver.find_element('xpath', update_xpath).text
        money_sh = float(driver.find_element('xpath', hmx).text[:-2]) * 100
        money_sz = float(driver.find_element('xpath', zmx).text[:-2]) * 100
        sum_north = float(driver.find_element('xpath', nsx).text[:-2]) * 100

        driver.close()

        dic_ = {'trade_date': [pd.to_datetime(update)],
                'NET_INFLOW_SH': [money_sh],
                'NET_INFLOW_SZ': [money_sz],
                'NET_INFLOW_BOTH': [sum_north]}

        df = pd.DataFrame(data=dic_)

        print(f'ä¸œæ–¹è´¢å¯Œä¸‹è½½{update}æ—¥åŒ—å‘èµ„é‡‘æˆåŠŸ;')
        return df

    @classmethod
    def funds_to_sectors(cls, date_: str):

        """
        north funds to sectors code_data
        """

        headers = my_headers('funds_to_sectors')

        url = my_url('funds_to_sectors').format(date_)
        # print(url)
        # exit()
        source_ = page_source(url=url, headers=headers)

        try:
            p1 = re.compile(r'[(](.*?)[)]', re.S)

            page_data = re.findall(p1, source_)
            json_data = json.loads(page_data[0])
            json_data = json_data['result']['code_data']
            # print(json_data)
            value_list = []
            for i in range(len(json_data)):
                values = list(json_data[i].values())
                value_list.append(values)

            key_list = list(json_data[0])

            df = pd.DataFrame(data=value_list, columns=key_list)

            columns = ['SECURITY_CODE', 'BOARD_CODE', 'BOARD_NAME', 'TRADE_DATE', 'COMPOSITION_QUANTITY',
                       'ADD_MARKET_CAP', 'BOARD_VALUE', 'HK_VALUE', 'HK_BOARD_RATIO', 'MAXADD_SECURITY_CODE',
                       'MAXADD_SECURITY_NAME', 'MINADD_SECURITY_CODE', 'MINADD_SECURITY_NAME',
                       'MAXADD_RATIO_SECURITY_NAME', 'MAXADD_RATIO_SECURITY_CODE',
                       'MINADD_RATIO_SECURITY_NAME', 'MINADD_RATIO_SECURITY_CODE']

            df = df[columns]

            df['TRADE_DATE'] = pd.to_datetime(df['TRADE_DATE']).dt.date

        except TypeError:
            print(f'ä¸œæ–¹è´¢å¯Œä¸‹è½½ Funds to Sectors æ•°æ®å¼‚å¸¸;')
            df = pd.DataFrame(data=None)

        return df

    @classmethod
    def industry_list(cls):  # ä¸‹è½½æ¿å—ç»„æˆ
        web = 'http://quote.eastmoney.com/center/boardlist.html#industry_board'
        driver = webdriver.Chrome()
        driver.get(web)

        source = driver.page_source
        bs_data = soup(source, 'html.parser')
        board_data = bs_data.find('li', class_='sub-items menu-industry_board-wrapper')
        board_data = board_data.find_all('li')
        data = pd.DataFrame(data=None)

        for i in range(len(board_data)):
            board_name = board_data[i].find(class_='text').text
            board_code = str(board_data[i].find('a')['href']).strip()[-6:]
            data.loc[i, 'board_name'] = board_name
            data.loc[i, 'board_code'] = board_code

        driver.close()
        data['stock_name'] = None
        data['stock_code'] = None
        return data

    @classmethod
    def industry_ind_stock(cls, name, code, num=300):  # ä¸‹è½½æ¿å—æˆä»½è‚¡
        url = my_url('industry_ind_stock').format(num, code)
        headers = my_headers('industry_ind_stock')
        source = page_source(url=url, headers=headers)

        dl = None
        if source:
            p1 = re.compile(r'[(](.*?)[)]', re.S)
            page_data = re.findall(p1, source)
            json_data = json.loads(page_data[0])['code_data']['diff']

            values_list = []
            for i in range(len(json_data)):
                values = list(json_data[i].values())
                values_list.append(values)

            key_list = list(json_data[0])
            dl = pd.DataFrame(data=values_list, columns=key_list)

            rename_ = {'f3': 'æ¶¨è·Œå¹…', 'f4': 'æ¶¨è·Œé¢', 'f5': 'æˆäº¤é‡', 'f6': 'æˆäº¤é¢',
                       'f7': 'æŒ¯å¹…', 'f8': 'æ¢æ‰‹ç‡', 'f9': 'å¸‚ç›ˆç‡åŠ¨', 'f10': 'é‡æ¯”',
                       'f12': 'stock_code', 'f14': 'stock_name', 'f15': 'close',
                       'f16': 'low', 'f17': 'open', 'f18': 'preclose', 'f20': 'æ€»å¸‚å€¼',
                       'f21': 'æµé€šå¸‚å€¼', 'f23': 'å¸‚å‡€ç‡', 'f115': 'å¸‚ç›ˆç‡'}

            dl = dl.rename(columns=rename_)

            dl = dl.drop(columns=['f1', 'f2', 'f11', 'f13', 'f22', 'f24', 'f25', 'f45',
                                  'f62', 'f128', 'f140', 'f141', 'f136', 'f152'])

            dl['board_name'] = name
            dl['board_code'] = code
            dl['date'] = pd.Timestamp('today').date()

            dl = dl[['board_name', 'board_code', 'stock_code', 'stock_name', 'date']]

        return dl

    @classmethod
    def funds_awkward_api(cls, code):
        """
        å°è¯•ä½¿ç”¨APIæ¥å£è·å–åŸºé‡‘æŒä»“æ•°æ®
        """
        try:
            # å°è¯•ä¸œæ–¹è´¢å¯ŒåŸºé‡‘æŒä»“API
            api_url = f"http://fund.eastmoney.com/data/fbsfundranking.html"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',
                'Accept': 'application/json, text/javascript, */*; q=0.01',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Referer': f'http://fund.eastmoney.com/{code}.html',
                'X-Requested-With': 'XMLHttpRequest'
            }
            
            logging.info(f"å°è¯•APIæ–¹å¼è·å–åŸºé‡‘ {code} æ•°æ®")
            
            # å°è¯•ä¸åŒçš„APIç«¯ç‚¹
            api_endpoints = [
                f"http://fund.eastmoney.com/api/FundPosition/{code}",
                f"http://fund.eastmoney.com/api/FundHoldings/{code}",
                f"http://fund.eastmoney.com/data/fbsfundranking.html?ft={code}",
            ]
            
            for api_url in api_endpoints:
                try:
                    source = page_source(url=api_url, headers=headers)
                    if source and len(source) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å®¹
                        logging.info(f"API {api_url} è¿”å›æ•°æ®é•¿åº¦: {len(source)}")
                        
                        # å°è¯•è§£æJSONæ•°æ®
                        try:
                            import json
                            data = json.loads(source)
                            logging.info(f"æˆåŠŸè§£æJSONæ•°æ®: {type(data)}")
                            return cls._parse_api_data(data, code)
                        except json.JSONDecodeError:
                            logging.warning(f"APIè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆJSON: {api_url}")
                            continue
                        
                except Exception as e:
                    logging.warning(f"API {api_url} è¯·æ±‚å¤±è´¥: {e}")
                    continue
            
            logging.warning(f"æ‰€æœ‰APIå°è¯•éƒ½å¤±è´¥äº†")
            return pd.DataFrame()
            
        except Exception as e:
            logging.error(f"APIæ–¹å¼è·å–åŸºé‡‘ {code} æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return pd.DataFrame()
    
    @classmethod
    def _parse_api_data(cls, data, code):
        """è§£æAPIè¿”å›çš„æ•°æ®"""
        try:
            li_name = []
            li_code = []
            
            # å°è¯•ä¸åŒçš„æ•°æ®ç»“æ„
            if isinstance(data, dict):
                # æŸ¥æ‰¾å¯èƒ½åŒ…å«è‚¡ç¥¨æ•°æ®çš„å­—æ®µ
                possible_fields = ['data', 'result', 'list', 'stocks', 'positions', 'holdings']
                
                for field in possible_fields:
                    if field in data:
                        items = data[field]
                        if isinstance(items, list):
                            for item in items:
                                if isinstance(item, dict):
                                    # å°è¯•æå–è‚¡ç¥¨åç§°å’Œä»£ç 
                                    stock_name = item.get('name') or item.get('stock_name') or item.get('title')
                                    stock_code = item.get('code') or item.get('stock_code') or item.get('id')
                                    
                                    if stock_name and stock_code:
                                        li_name.append(str(stock_name))
                                        li_code.append(str(stock_code))
                                        logging.info(f"ä»APIè§£æåˆ°è‚¡ç¥¨: {stock_name} ({stock_code})")
            
            if li_name and li_code:
                dic = {'stock_name': li_name, 'stock_code': li_code}
                return pd.DataFrame(dic)
            else:
                logging.warning(f"APIæ•°æ®ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„è‚¡ç¥¨ä¿¡æ¯")
                return pd.DataFrame()
                
        except Exception as e:
            logging.error(f"è§£æAPIæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return pd.DataFrame()

    @classmethod
    def funds_awkward(cls, code):
        """
        è·å–åŸºé‡‘æŒä»“æ•°æ®ï¼Œå°è¯•å¤šç§æ–¹æ³•
        """
        # é¦–å…ˆå°è¯•APIæ–¹æ³•
        data = cls.funds_awkward_api(code)
        if not data.empty:
            logging.info(f"APIæ–¹æ³•æˆåŠŸè·å–åŸºé‡‘ {code} æ•°æ®")
            return data
        
        # å¦‚æœAPIå¤±è´¥ï¼Œå°è¯•ç½‘é¡µè§£ææ–¹æ³•
        logging.info(f"APIæ–¹æ³•å¤±è´¥ï¼Œå°è¯•ç½‘é¡µè§£ææ–¹æ³•")
        return cls.funds_awkward_web(code)

    @classmethod
    def funds_awkward_web(cls, code):
        """
        ä»ç½‘é¡µè§£æåŸºé‡‘æŒä»“æ•°æ®
        """
        try:
            url = my_url('funds_awkward').format(code)
            headers = my_headers('funds_awkward')
            
            # æ£€æŸ¥URLæ˜¯å¦ä¸ºç©º
            if not url:
                logging.error(f"åŸºé‡‘ {code} çš„URLé…ç½®ä¸ºç©º")
                return pd.DataFrame()
            
            logging.info(f"æ­£åœ¨ä¸‹è½½åŸºé‡‘ {code} çš„æ•°æ®ï¼ŒURL: {url}")
            
            source = page_source(url=url, headers=headers)
            
            # æ£€æŸ¥é¡µé¢æºç æ˜¯å¦ä¸ºç©º
            if not source:
                logging.error(f"åŸºé‡‘ {code} é¡µé¢æºç ä¸ºç©º")
                return pd.DataFrame()
            
            # è§£æHTML
            soup_obj = soup(source, 'html.parser')
            if not soup_obj:
                logging.error(f"åŸºé‡‘ {code} HTMLè§£æå¤±è´¥")
                return pd.DataFrame()
            
            # æŸ¥æ‰¾æ‰€æœ‰è‚¡ç¥¨é“¾æ¥
            stock_links = soup_obj.find_all("a", href=lambda href: href and '/stock/' in href)
            
            if not stock_links:
                logging.warning(f"åŸºé‡‘ {code} æœªæ‰¾åˆ°è‚¡ç¥¨é“¾æ¥")
                return pd.DataFrame()
            
            li_name = []
            li_code = []
            
            for link in stock_links:
                try:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    
                    # ä»é“¾æ¥ä¸­æå–è‚¡ç¥¨ä»£ç 
                    if '/stock/' in href:
                        # æå–è‚¡ç¥¨ä»£ç 
                        code_match = href.split('/stock/')[-1].split('.')[0]
                        if len(code_match) == 6 and code_match.isdigit():
                            stock_code = code_match
                            stock_name = text
                            
                            # éªŒè¯è‚¡ç¥¨åç§°ï¼ˆåº”è¯¥æ˜¯ä¸­æ–‡ï¼‰
                            if stock_name and len(stock_name) > 0:
                                # æ¸…ç†æ–‡æœ¬
                                stock_name = stock_name.replace('\n', '').replace('\r', '').replace('\t', '').strip()
                                
                                li_name.append(stock_name)
                                li_code.append(stock_code)
                                logging.info(f"åŸºé‡‘ {code} è§£æåˆ°è‚¡ç¥¨: {stock_name} ({stock_code})")
                    
                except Exception as e:
                    logging.warning(f"è§£æåŸºé‡‘ {code} è‚¡ç¥¨é“¾æ¥æ—¶å‡ºé”™: {e}")
                    continue
            
            if not li_name or not li_code:
                logging.warning(f"åŸºé‡‘ {code} æœªè§£æåˆ°æœ‰æ•ˆçš„è‚¡ç¥¨æ•°æ®")
                return pd.DataFrame()

            dic = {'stock_name': li_name, 'stock_code': li_code}
            data = pd.DataFrame(dic)
            
            logging.info(f"åŸºé‡‘ {code} æˆåŠŸä¸‹è½½ {len(data)} æ¡è‚¡ç¥¨æ•°æ®")
            return data
            
        except Exception as e:
            logging.error(f"ä¸‹è½½åŸºé‡‘ {code} æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return pd.DataFrame()


if __name__ == '__main__':
    download = DownloadData.stock_1m_1day('002475')
    print(download)
